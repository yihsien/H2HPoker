\documentclass[12pt]{article}

%% http://en.wikibooks.org/wiki/LaTeX/Page_Layout#Margins
\usepackage[margin=1in]{geometry}

\input{latex-config}

%% Suppress page numbering
\pagenumbering{gobble}
%\pagenumbering{roman}

\begin{document}

\title{Exploration of Near-optimal strategies for Poker}
\author{
	Yi-Hsien Lin \\ yishien@princeton.edu \and
	Akshay Mittal \\ akshay@princeton.edu
}
\date{}

\maketitle

\section{Introduction}

\section{Background Study}

\section{Overview of Texas Hold'em Poker}

\section{Game Setup}

\section{Randomized Weighted Majority Algorithm}
\label{sec:rwma}
\noindent In the algorithm, the experts are predicting whether the learner should call, raise, check
or fold and the learner makes a move by choice of the randomized weighted majority
among the experts. The observed label $y_t$ (win/loss) is then used to update
the weights of all the experts. We keep track of the sum of the weights of all the $N$ experts $W_t =
\Sum_{i=1}^Nw_{t,i}$ on each round $t$. We define $q_{t,move=\{c,r,k,f\}}$ as the sum of the weights
of the experts predicting the corresponding move at round $t$ \ie \[q_{t,move}=\Sum_{i:\xi_{t,i}=move}w_{t,i}\]
Initially let $W_1 = N$,
and on each round $t$, the probability (over the algorithm's randomization) that the algorithm makes a mistake is
\begin{align}
l_t &= Pr[\hat{y}_t\neq y_t]\tag*{}\\
&=\begin{cases}
  	(q_{t,c} + q_{t, r})/W_t & \text{if}~y_{t}=lost~or~0\\
  	(q_{t,k} + q_{t, f})/W_t & \text{if}~y_{t}=won~or~1\\
	\end{cases}
\label{eq:prob-mistake}
\end{align}
\noindent The intuition behind the above probability is that when the learner loses
a round of poker, then all the experts, who made the learner stay in the game by paying
additional money, should be penalized and their weights be reduced. This corresponds to
the experts that predict \texttt{call} or \texttt{raise} since the observation is a lost
game and the learner loses money. By a similar argument, in the case of a win observation,
the weights of the experts, who made the learner leave the game and lost the opportunity
to make more money, are penalized, precisely the experts that predict \texttt{fold} or
\texttt{check}.\\

\noindent In order to relate the predictions of the experts (in terms of {\em moves}) with the
observations (result of game) of the learner, we define the {\em equality} operator
$=^*$ as follows
\begin{align*}
call &\neq^* lost\\
raise &\neq^* lost\\
fold &\neq^* won\\
check &\neq^* won
\end{align*}
\noindent Using the above inequalities, we can rewrite Equation~\ref{eq:prob-mistake} as follows
\begin{align}
l_t = \Sum_{i:\xi_{t,i}\neq^*y_t}w_{t,i}/W_t
\end{align}
\noindent Therefore, we can compute the new sum of weights as follows
\begin{align}
W_{t+1} &= \Sum_{i:\xi_{t,i}\neq^*y_t}w_{t+1,i} + \Sum_{i:\xi_{t,i}=^*y_t}w_{t+1,i}\tag*{}\\
&= \Sum_{i:\xi_{t,i}\neq^*y_t}w_{t,i}\beta + \Sum_{i:\xi_{t,i}=^*y_t}w_{t,i}
\label{eq:same-beta}
\end{align}
\noindent In Equation~\ref{eq:same-beta}, it is assumed the experts are penalized with the same
weight factor $\beta$ irrespective of the mis-predictions made on \texttt{call}, \texttt{raise},
\texttt{fold} or \texttt{check}. This leads to a bound similar to the randomized weighted
majority algorithm \ie
\[\mathbb{E}[(\#\text{mistakes of learner})]\leq a_{\beta}\mathbb{E}[(\#\text{mistakes of best expert})]
+ c_{\beta}\lg N\] where $a_{\beta}=\frac{\ln(1/\beta)}{1-\beta}$ and $c_{\beta}=\frac{1}{1-\beta}$.\\

\noindent We consider the above bound as a bound for the {\em naive} approach since it provides
same penalty for making mistakes on different moves. We attempt to lose this assumption by
modeling the poker experts in a more realistic manner. Let $\beta_{move=\{c, r, f, k\}}$ be the
penalty factor for the expert making mistake on the corresponding move. When the learner loses
a round, then the expert which predicted a \texttt{raise} should be penalized more than the expert
predicting a \texttt{call} since the former caused a greater loss of money. Similarly, when the learner
wins a round, then the expert which predicted a \texttt{fold} should be penalized more than the expert
predicting a \texttt{check} since the former caused the learner to quit and lose the opportunity to gain
money by staying in the game. Therefore, let there exist $\epsilon>0$ such that we let $\beta_c + \epsilon
= \beta_r$ and $\beta_k + \epsilon = \beta_f$. Equation~\ref{eq:same-beta} can thus be re-framed for the following
two cases
\subsubsection*{Case: $y_t = lost$}
\begin{align}
W_{t+1} &= \Sum_{i:\xi_{t,i}=c}w_{t,i}\beta_c + \Sum_{i:\xi_{t,i}=r}w_{t,i}\beta_r + \Sum_{i:\xi_{t,i}=^*y_t}w_{t,i}\tag*{}\\
&= \beta_cq_{t,c} + \beta_rq_{t,r} + \Sum_{i:\xi_{t,i}=^*y_t}w_{t,i}\tag*{}\\
&= \beta_c(q_{t,c}+q_{t,r}) + \epsilon q_{t,r} + \Sum_{i:\xi_{t,i}=^*y_t}w_{t,i}\tag*{}\\
&= \beta_cl_tW_t + \epsilon q_{t,r} + (W_t - l_tW_t)\tag*{}\\
&= W_t(1-l_t(1\beta_c)) + \epsilon q_{t,r}
\end{align}

\section{Winnow Algorithm}

\section{Related Work}

\section{Experimentation}

\section{Conclusion}

\subsection{Future Work}

\end{document}
